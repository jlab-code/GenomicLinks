{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75673b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gffutils.iterators import DataIterator\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import h5py\n",
    "import string\n",
    "from random import choice\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24526ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "\n",
    "    ##############################################\n",
    "    # Main Run function\n",
    "    ##############################################\n",
    "    def __init__(self, data_dir, HiChip_marks_list, num_seq_signals_excluding_histones,\n",
    "                    HDF5_samples_num, using_fasta_seq, window_size):\n",
    "        os.chdir(data_dir)\n",
    "        start = time.process_time()\n",
    "        \n",
    "        # create class variable for window_size\n",
    "        self.window_size = window_size\n",
    "    \n",
    "        # data size\n",
    "        # Create gene regions file (includes indices of genes corresponding chromosome number)\n",
    "        self.input_file_gff, self.output_file_gff = self.create_output_genes_file()\n",
    "        print(\"Reference Genome: \", self.input_file_gff, \"Output file: \", self.output_file_gff)\n",
    "        self.genome_lenght, self.seq_regions_maize = self.genome_lenght(self.input_file_gff)\n",
    "\n",
    "        # display genome length and Chromosome lengths\n",
    "        print(self.genome_lenght, 'is the genome lenght')\n",
    "        print(self.seq_regions_maize, 'are the lenghts of each chromosome by order')\n",
    "        print(len(self.seq_regions_maize), 'are the total number of chromosomes')\n",
    "\n",
    "        # get matrix for centers of anchors\n",
    "        self.anchor_centers_matrix = self.get_centers_matrix(data_dir + \"7205_SupTable_Loops.xlsx\")\n",
    "        print(\"Resulting centers matrix has shape: \", self.anchor_centers_matrix.shape)\n",
    "\n",
    "        # process excel\n",
    "        self.list_filename_data, self.list_filename_headers = self.process_excel_file(data_dir + \"7205_SupTable_Loops.xlsx\")\n",
    "\n",
    "        print(\"filename_data: \")\n",
    "        print(self.list_filename_data)\n",
    "        print(\"filename_headers: \")\n",
    "        print(self.list_filename_headers)\n",
    "\n",
    "        # get positive samples HiChip_marks_data (creating the positive set with chr,s1,e1,s2,e2 window_size\n",
    "        self.positive_set = self.generate_positive_set(self.anchor_centers_matrix)\n",
    "        print(\"The specified window size is: \", self.window_size)\n",
    "        print(\"Resulting window adjusted positive set matrix has shape: \", self.positive_set.shape)\n",
    "\n",
    "\n",
    "        # get negative samples \n",
    "        self.negative_set = self.generate_negative_set(self.anchor_centers_matrix)\n",
    "\n",
    "        # get combined dataset (combine negative samples with positives and shuffle)\n",
    "        self.neg_and_pos_combined_set = self.combine_negative_positive_sets(self.positive_set, self.negative_set)\n",
    "        \n",
    "        # print generated matrix info\n",
    "        print(self.positive_set.shape, 'is the positive set lenght')\n",
    "        print(self.negative_set.shape, 'is the negative set lenght')\n",
    "        print(self.neg_and_pos_combined_set.shape, 'is the combined set lenght')\n",
    "\n",
    "\n",
    "        # create Histone samples of the data\n",
    "        self.total_num_seq_signals = num_seq_signals_excluding_histones\n",
    "        print(self.total_num_seq_signals, 'is total_num_seq')\n",
    "        print(HDF5_samples_num, 'HDF5')\n",
    "        \n",
    "        # saving to hdf5 file\n",
    "        self.h5_filename = self.data_to_h5(self.neg_and_pos_combined_set,\n",
    "                                             self.total_num_seq_signals, HDF5_samples_num,\n",
    "                                             using_fasta_seq )\n",
    "        print('Data Preprocessing Finished.')\n",
    "        print('Code execution time:', time.process_time() - start)\n",
    "        \n",
    "        \n",
    "    ##############################################\n",
    "    # Class Functions\n",
    "    ##############################################\n",
    "    def replace_chr_text_in_file(self, filename):\n",
    "        # Read in the file\n",
    "        with open(filename, 'r') as file:\n",
    "            file_content = file.read()\n",
    "        # Replace the target string\n",
    "        pattern = re.compile(\"chr\", re.IGNORECASE)\n",
    "        file_content = pattern.sub(\"\", file_content)\n",
    "\n",
    "        # Write the file out again\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(file_content)\n",
    "\n",
    "    # Calculate file length\n",
    "    def reg_file_len(self, filename):\n",
    "        with open(filename) as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        return i + 1\n",
    "    \n",
    "    def create_output_genes_file(self):\n",
    "        if len(glob.glob(\"*.gff\")) > 0:\n",
    "            input_file_gff = glob.glob(\"*.gff\")[0]\n",
    "        elif len(glob.glob(\"*.gff3\")) > 0:\n",
    "            input_file_gff = glob.glob(\"*.gff3\")[0]\n",
    "        else:\n",
    "            print(\"no GFF file found\")\n",
    "\n",
    "        if not os.path.exists(\"output_genes.txt\"):\n",
    "            with open(str(\"output_genes.txt\"), 'w') as fout:\n",
    "                for feature in DataIterator(str(input_file_gff)):\n",
    "                    if feature.featuretype == \"gene\":\n",
    "                        fout.write(str(feature[0]) + '\\t' + str(feature[3]) + '\\t' + str(feature[4]) + '\\t' +\n",
    "                                   (feature[8][\"gene_id\"])[0] + '\\n')\n",
    "            self.replace_chr_text_in_file(str(\"output_genes.txt\"))\n",
    "\n",
    "        output_file_gff = \"output_genes.txt\"\n",
    "        return input_file_gff, output_file_gff\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: Calculate the lenght of the genome using the GFF file\n",
    "    ##############################################\n",
    "    #safe cast\n",
    "    def safe_cast(self, val, to_type, default=None):\n",
    "        try:\n",
    "            return to_type(val)\n",
    "        except (ValueError, TypeError):\n",
    "            return default\n",
    "\n",
    "    def genome_lenght(self, input_file_gff):\n",
    "        seq_regions_maize_tmp = []\n",
    "        list_order = []\n",
    "        with open(input_file_gff) as f_original_gff:\n",
    "            content_original_gff = f_original_gff.readlines()\n",
    "\n",
    "        for hashtag_row in range(self.reg_file_len(input_file_gff)):\n",
    "            if content_original_gff[hashtag_row].startswith('##'):\n",
    "                if content_original_gff[hashtag_row].startswith('##sequence-region'):\n",
    "                    temp_region = content_original_gff[hashtag_row].split()\n",
    "                    if type(self.safe_cast(temp_region[1], int)) == int or str(temp_region[1]).startswith(\"chr\"):\n",
    "                        seq_regions_maize_tmp.append(self.safe_cast(temp_region[3], int))\n",
    "                        list_order.append(self.safe_cast(temp_region[1], int) - 1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        seq_regions_maize = [None] * len(seq_regions_maize_tmp)\n",
    "        for c, elem in enumerate(list_order):\n",
    "            seq_regions_maize[elem] = seq_regions_maize_tmp[c]\n",
    "\n",
    "        return sum(seq_regions_maize), seq_regions_maize\n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    # Function: get centers matrix for anchors\n",
    "    # @author : Defne\n",
    "    ##############################################\n",
    "    def get_centers_matrix(self, dataset_path):\n",
    "\n",
    "        # Load spreadsheet: xlsx\n",
    "        xlsx = pd.ExcelFile(dataset_path)\n",
    "        all_sheet_dfs = []\n",
    "        all_names = []\n",
    "        #process the sheet names as necessary\n",
    "        for name in xlsx.sheet_names[1:]:\n",
    "            sheet = pd.read_excel(dataset_path, sheet_name = name)\n",
    "            sheet_temp = sheet.drop(labels = [0,1,2,3,4], axis = 0)\n",
    "            columns_to_keep = [x for x in range(sheet_temp.shape[1]) if x in [0,1,2,3,4]]\n",
    "            sheet_temp = sheet_temp.iloc[:, columns_to_keep]\n",
    "            all_sheet_dfs.append(sheet_temp)\n",
    "            all_names.append(name)\n",
    "\n",
    "\n",
    "        #get center matrices as dfs\n",
    "        center_dfs = []\n",
    "        for sheet_df in all_sheet_dfs:\n",
    "            center_df = self.get_anchor_centers(sheet_df)\n",
    "            center_dfs.append(center_df)\n",
    "\n",
    "        #create a matrix from centered data and return that\n",
    "        centers_matrix = pd.concat(center_dfs)\n",
    "        centers_matrix_np = np.array(centers_matrix)    #return matrix as numpy\n",
    "\n",
    "        return centers_matrix_np\n",
    "    \n",
    "    \n",
    "    ##############################################\n",
    "    # Helper function: get anchor centers\n",
    "    # @author: Defne\n",
    "    ##############################################\n",
    "    def get_anchor_centers(self, dataset_df):\n",
    "\n",
    "        center1_list = []\n",
    "        center2_list = []\n",
    "        chr_no_list = dataset_df.iloc[:,0].tolist()\n",
    "\n",
    "        for index, row in dataset_df.iterrows():\n",
    "            center1_list.append(int((row.iloc[1] + row.iloc[2]) / 2))\n",
    "            center2_list.append(int((row.iloc[3] + row.iloc[4]) / 2))\n",
    "\n",
    "        centers_matrix_column_list = list(zip(chr_no_list, center1_list, center2_list))\n",
    "\n",
    "        anchor_centers_matrix = pd.DataFrame(centers_matrix_column_list)\n",
    "        print(anchor_centers_matrix[1:5])\n",
    "        return anchor_centers_matrix\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: process excel file\n",
    "    # @author: Defne\n",
    "    ##############################################\n",
    "    def process_excel_file(self, dataset_path):\n",
    "        #get assay data from excel according to leaf names\n",
    "        #and create lists of names and headers\n",
    "        #output: two list objects for headers and names\n",
    "        #input: an xlsx file\n",
    "        #output: list - filename_data, list - filename_header\n",
    "\n",
    "        # Load spreadsheet: xls\n",
    "        xlsx = pd.ExcelFile(dataset_path)\n",
    "\n",
    "        #get sheet names and headers\n",
    "        list_filename_data = []\n",
    "        list_filename_headers = []\n",
    "\n",
    "        #process the sheet names as necessary\n",
    "        for name in xlsx.sheet_names[1:]:\n",
    "            d_mark = name.split('_')[0]\n",
    "            h_mark = d_mark + \"_2500_headers.txt\"\n",
    "            d_mark = d_mark + \"_2500_data.txt\"\n",
    "\n",
    "            if not os.path.isfile(d_mark) or not os.path.isfile(h_mark):\n",
    "                d = pd.read_excel(dataset_path, sheet_name = name)\n",
    "                h = d.iloc[4].tolist()\n",
    "                h = h[0:5]\n",
    "                d = d.drop(range(0,5))\n",
    "\n",
    "                columns_to_keep = [x for x in range(d.shape[1]) if x in [0,1,2,3,4]]\n",
    "                d = d.iloc[:, columns_to_keep]\n",
    "\n",
    "                #get header names and create the headers.txt file\n",
    "                d.columns = h\n",
    "\n",
    "                #save the data and headers in .txt files\n",
    "                np.savetxt(d_mark, d.values, delimiter = \"\\t \", fmt = '%s')\n",
    "\n",
    "            list_filename_data.append(d_mark)\n",
    "            list_filename_headers.append(h_mark)\n",
    "\n",
    "        return list_filename_data, list_filename_headers\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: Generate positive labled set using\n",
    "    # HiChip marks data (also does the window_size \n",
    "    # adjustment)\n",
    "    # @author: Defne\n",
    "    ##############################################\n",
    "    def generate_positive_set(self, anchor_centers_matrix):\n",
    "    \n",
    "        #for convinience - change data to dataframe\n",
    "        centers_df = pd.DataFrame(anchor_centers_matrix)\n",
    "\n",
    "        s1_list = []\n",
    "        e1_list = []\n",
    "        s2_list = []\n",
    "        e2_list = []\n",
    "        chr_no_list = centers_df.iloc[:,0].tolist()\n",
    "        #window_size = 2500\n",
    "\n",
    "        for index, row in centers_df.iterrows():\n",
    "            s1_list.append(int(row.iloc[1] - int(self.window_size/2)))\n",
    "            e1_list.append(int(row.iloc[1] + int(self.window_size/2)))\n",
    "\n",
    "            s2_list.append(int(row.iloc[2] - int(self.window_size/2)))\n",
    "            e2_list.append(int(row.iloc[2] + int(self.window_size/2)))\n",
    "\n",
    "        matrix_column_list = list(zip(chr_no_list, s1_list, e1_list, s2_list, e2_list))\n",
    "        \n",
    "        window_size_adjusted_positive_matrix = pd.DataFrame(matrix_column_list).to_numpy()\n",
    "\n",
    "        # create labels \n",
    "        positive_labels = np.full(len(adjusted_to_original_matrix), 1)\n",
    "        positive_labels = positive_labels.reshape(len(positive_labels), -1)\n",
    "        window_size_adjusted_positive_matrix = np.append(window_size_adjusted_positive_matrix, positive_labels, axis=1)\n",
    "        \n",
    "        print('positive set first rows', window_size_adjusted_positive_matrix[0:5])\n",
    "        \n",
    "        return window_adjusted_pos_matrix\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: Adjusting dataset from centers \n",
    "    # data to s1,e1,s2,e2 window\n",
    "    # @author: Defne\n",
    "    ##############################################\n",
    "    def adjust_negative_set_to_original_window(self, centers_matrix):\n",
    "\n",
    "        #for convinience - change data to dataframe\n",
    "        centers_df = pd.DataFrame(centers_matrix)\n",
    "\n",
    "        s1_list = []\n",
    "        e1_list = []\n",
    "        s2_list = []\n",
    "        e2_list = []\n",
    "        chr_no_list = centers_df.iloc[:,0].tolist()\n",
    "        #window_size = 2500\n",
    "\n",
    "        #loop through the center coordinates and adjust s1,e1,s2,e2 coordinates\n",
    "        for index, row in centers_df.iterrows():\n",
    "            s1_list.append(int(row.iloc[1] - int(self.window_size/2)))\n",
    "            e1_list.append(int(row.iloc[1] + int(self.window_size/2)))\n",
    "\n",
    "            s2_list.append(int(row.iloc[2] - int(self.window_size/2)))\n",
    "            e2_list.append(int(row.iloc[2] + int(self.window_size/2)))\n",
    "\n",
    "        matrix_column_list = list(zip(chr_no_list, s1_list, e1_list, s2_list, e2_list))\n",
    "\n",
    "        window_size_adjusted_negative_matrix = pd.DataFrame(matrix_column_list).to_numpy()\n",
    "        #change matrix to numpy\n",
    "        #adjusted_to_original_matrix = adjusted_to_original_matrix.to_numpy()\n",
    "\n",
    "        #label with all negatives (zeros)\n",
    "        negative_labels = np.full(len(adjusted_to_original_matrix), 0)\n",
    "        negative_labels = negative_labels.reshape(len(negative_labels), -1)\n",
    "        window_size_adjusted_negative_matrix = np.append(window_size_adjusted_negative_matrix, negative_labels, axis=1)\n",
    "        print('negative set first rows', window_size_adjusted_negative_matrix[0:5])\n",
    "        \n",
    "        #print(adjusted_to_original_matrix)\n",
    "\n",
    "        return window_size_adjusted_negative_matrix\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: generate negative set with window \n",
    "    # adjustment\n",
    "    # @author: Luca\n",
    "    ##############################################\n",
    "\n",
    "    def generate_negative_set(self,anchor_centers_matrix):\n",
    "        anchor_centers_matrix = anchor_centers_matrix[np.unique(anchor_centers_matrix[:, [0, 1, 2]], return_index=True,axis=0)[1]]\n",
    "        chr_column = anchor_centers_matrix[:,0]\n",
    "        max_chr = max(chr_column)\n",
    "        negative_set = np.zeros([0,3], dtype = int)\n",
    "        min_dist = 0\n",
    "        max_dist = 300000000#200000\n",
    "        stop = 100000#000\n",
    "        # go on here \n",
    "        #anchor_list = np.append(anchor_centers_matrix[:, [0, 1]], anchor_centers_matrix[:, [0, 2]])\n",
    "        print('ANCHOR CENTER MATRIX:', anchor_centers_matrix)\n",
    "        \n",
    "        #if not os.path.exists(\"negative_training_set.txt\"):\n",
    "        for i in range(1,max_chr+1):\n",
    "            print('Handling chr ', i)\n",
    "            submatrix = anchor_centers_matrix[anchor_centers_matrix[:,0] == i]\n",
    "            max_chr_idx = len(submatrix[:,0])\n",
    "            print('-------')\n",
    "            submatrix_permutated = submatrix.copy()\n",
    "            submatrix_permutated[:,2] = 0\n",
    "            for j in range(0,max_chr_idx):        \n",
    "                check_distance = False\n",
    "                es = 0\n",
    "                while check_distance == False:\n",
    "                    if es > stop:\n",
    "                        sys.exit(\"Error: Max distance is too small.\")\n",
    "                    # choose a random integer excluding j - no overlap with positive set.\n",
    "                    #random_index2 = choice([x for x in range(0,max_chr_idx) if x not in [j]])\n",
    "                    lower_index = random.randint(2,max_chr_idx)#(2,50)\n",
    "                    upper_index = random.randint(2,max_chr_idx)#(2,50)\n",
    "                    random_index3 = choice([x for x in range(max(0,j-lower_index),min(j+upper_index,max_chr_idx)) if x not in [j]])\n",
    "                    anchor1 = submatrix[j,1]\n",
    "                    anchor2 = submatrix[random_index3,2]\n",
    "                    random_shift = choice([x for x in range(-25,25) if x not in range(-5,5)])\n",
    "                    random_shift2 = choice([x for x in range(-25,25) if x not in range(-5,5)])\n",
    "                    distance = abs(anchor2 - anchor1)\n",
    "                    if distance > min_dist and distance < max_dist:                        \n",
    "                        submatrix_permutated[j,2]= anchor2\n",
    "                        submatrix_permutated[j,2] += random_shift\n",
    "                        submatrix_permutated[j,1] += random_shift2\n",
    "                        check_distance = True   \n",
    "                    else:\n",
    "                        es = es + 1            \n",
    "            negative_set = np.concatenate([negative_set,submatrix_permutated])        \n",
    "        print('negative_set:\\n', negative_set[0:10])\n",
    "        np.savetxt('negative_training_set.txt',negative_set, fmt = '%i', delimiter = '\\t')        \n",
    "        #negative_samples = negative_set\n",
    "        #size = negative_set.shape\n",
    "        print('dimension of the created negative set:', negative_set.shape)\n",
    "        window_adjusted_negative_set = self.adjust_negative_set_to_original_window(negative_set)\n",
    "        np.savetxt('negative_training_set.txt',window_adjusted_negative_set, fmt = '%i', delimiter = '\\t')\n",
    "\n",
    "        return window_adjusted_negative_set\n",
    "    \n",
    "    ##############################################\n",
    "    # Function: Combine negative samples with positives \n",
    "    # and shuffle\n",
    "    # Train & Test Samples\n",
    "    ##############################################\n",
    "    def combine_negative_positive_sets(self, positive_set, negative_set):\n",
    "        print(\"Combining positive and negative set.\")\n",
    "        print('received positive set:', positive_set[0:5])\n",
    "        print('received negative :', negative_set[0:5])\n",
    "        combined_set = np.concatenate((positive_set, negative_set))\n",
    "        # shuffle the negatives and positives\n",
    "        np.random.shuffle(combined_set)\n",
    "        print(\"len Shuffle:\", len(combined_set))\n",
    "        np.savetxt('training_set_shuffled.txt',combined_set, fmt = '%i', delimiter = '\\t')\n",
    "        return combined_set\n",
    "    \n",
    "    ##############################################\n",
    "    # process_FASTA_vector\n",
    "    ##############################################\n",
    "    def process_FASTA_vector(self, region_s, region_e, chr_num_in_line, retrieve_FASTA_matrix):\n",
    "\n",
    "        FASTA_sequences_filename = str(region_s) + \"_\" + str(region_e) + \".txt\"\n",
    "\n",
    "        if not os.path.exists(\"FASTA_sequences/\" + chr_num_in_line + \"/\" + FASTA_sequences_filename):\n",
    "            # create FASTA per_base_cov directory if not exists\n",
    "            if not os.path.exists(\"FASTA_sequences\"):\n",
    "                try:\n",
    "                    os.mkdir(\"FASTA_sequences\")\n",
    "                except OSError:\n",
    "                    print(\"Creation of the directory failed -- FASTA_sequences\")\n",
    "\n",
    "            if not os.path.exists(\"FASTA_sequences/\" + chr_num_in_line):\n",
    "                try:\n",
    "                    os.mkdir(\"FASTA_sequences/\" + chr_num_in_line)\n",
    "                except OSError:\n",
    "                    print(\"Creation of the directory failed -- \" + \"FASTA_sequences/\" + chr_num_in_line)\n",
    "\n",
    "            FASTA_sequences_filename = str(region_s) + \"_\" + str(region_e) + \".txt\"\n",
    "\n",
    "            if not os.path.exists(\"FASTA_sequences/\" + chr_num_in_line + \"/\" + FASTA_sequences_filename):\n",
    "                with open('temp_bash.sh', 'w') as the_file:\n",
    "                    the_file.write(\n",
    "                        \"bedtools getfasta -fi Zea_mays_B73_v4.fasta -bed temp_region.txt > FASTA_sequences/\" + chr_num_in_line + \"/\" + FASTA_sequences_filename)\n",
    "                subprocess.call([\"sh\", \"./temp_bash.sh\"])\n",
    "            \n",
    "        if retrieve_FASTA_matrix and os.path.exists(\n",
    "                \"FASTA_sequences/\" + chr_num_in_line + \"/\" + FASTA_sequences_filename):\n",
    "            FASTA_matrix = np.zeros((4, 2500))  # ACGT\n",
    "            print(chr_num_in_line)\n",
    "            print(FASTA_sequences_filename)\n",
    "            fp = open(\"FASTA_sequences/\" + chr_num_in_line + \"/\" + FASTA_sequences_filename)\n",
    "            for i, line in enumerate(fp):\n",
    "                if i == 1:\n",
    "                    line_character_count = 0\n",
    "                    for c in line:\n",
    "                        # print c\n",
    "                        if c == \"A\":\n",
    "                            FASTA_matrix[0][line_character_count] = 1\n",
    "                        if c == \"C\":\n",
    "                            FASTA_matrix[1][line_character_count] = 1\n",
    "                        if c == \"G\":\n",
    "                            FASTA_matrix[2][line_character_count] = 1\n",
    "                        if c == \"T\":\n",
    "                            FASTA_matrix[3][line_character_count] = 1\n",
    "                        if c == \"N\":\n",
    "                            FASTA_matrix[0][line_character_count] = 0.25\n",
    "                            FASTA_matrix[1][line_character_count] = 0.25\n",
    "                            FASTA_matrix[2][line_character_count] = 0.25\n",
    "                            FASTA_matrix[3][line_character_count] = 0.25\n",
    "                        line_character_count += 1\n",
    "            fp.close()\n",
    "\n",
    "            return FASTA_matrix\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # create Matrix\n",
    "    ##############################################\n",
    "    def create_matrix(self, chr_num, region_s, region_e, using_fasta_seq):\n",
    "\n",
    "        with open('temp_region.txt', 'w') as the_file:\n",
    "            the_file.write(str(chr_num) + \"\\t\" + str(region_s) + \"\\t\" + str(region_e))\n",
    "\n",
    "        #print(\"subprocess call will start\")\n",
    "\n",
    "        # empty the temp_bash.sh file\n",
    "        open('temp_bash.sh', 'w').close()\n",
    "\n",
    "        subprocess_bedtools_job_tracker_counter = 0\n",
    "\n",
    "        if not os.stat(\"temp_bash.sh\").st_size == 0:\n",
    "            for subprocess_count in range(subprocess_bedtools_job_tracker_counter):\n",
    "                with open('temp_bash.sh', 'a') as the_file:\n",
    "                    letter = string.ascii_lowercase[subprocess_count]\n",
    "                    the_file.write(\"wait $\" + letter + \"\\n\" \"echo \\\"job \" + letter + \" returned $?\\\" \\n\")\n",
    "                    the_file.close()\n",
    "\n",
    "        # subprocess call\n",
    "        subprocess.call([\"sh\", \"./temp_bash.sh\"])\n",
    "        #print(\"subprocess call ended\")\n",
    "\n",
    "        # generate sequence\n",
    "        if using_fasta_seq:\n",
    "            self.process_FASTA_vector(region_s, region_e, chr_num, retrieve_FASTA_matrix=False)\n",
    "\n",
    "        final_matrix = []\n",
    "\n",
    "        final_matrix = np.array(final_matrix, dtype=int)\n",
    "\n",
    "        # get ACGT sequence\n",
    "        if using_fasta_seq:\n",
    "            FASTA_matrix = self.process_FASTA_vector(region_s, region_e, chr_num, retrieve_FASTA_matrix=True)\n",
    "            if len(final_matrix) == 0:\n",
    "                final_matrix = FASTA_matrix\n",
    "            else:\n",
    "                final_matrix = np.array(final_matrix, dtype=int)\n",
    "                final_matrix = np.vstack((final_matrix, FASTA_matrix))\n",
    "\n",
    "        final_matrix = np.array(final_matrix, dtype=int)\n",
    "        #print('FINAL_CREATE_MATRIX MATRIX:', final_matrix)\n",
    "        #print(final_matrix.shape, \"final_matrix shape\")\n",
    "        return final_matrix\n",
    "\n",
    "\n",
    "    ##############################################\n",
    "    # Create hdf5 file of the data (sequence information and labels)\n",
    "    ##############################################\n",
    "    def data_to_h5(self, data_matrix_with_neg_and_pos,\n",
    "                                           total_num_seq_signals, HDF5_samples_num,\n",
    "                                           using_fasta_seq):\n",
    "\n",
    "        h5_filename = \"regions_labels_rand_neg_\" + str(HDF5_samples_num) + \"2500\" +\".h5\"\n",
    "        print('total: ', total_num_seq_signals)\n",
    "        #print('HDF5: ', HDF5_samples_num ) # this is 202344\n",
    "        print('using_fasta_seq: ', using_fasta_seq)\n",
    "        if not 1>2:#os.path.isfile(h5_filename):\n",
    "            hf = h5py.File(h5_filename, 'w')\n",
    "            hf.create_dataset('re_region_matrices', shape=(HDF5_samples_num, total_num_seq_signals, 2500), dtype=\"int\",\n",
    "                              data=None)\n",
    "            hf.create_dataset('interactions_region_matrices', shape=(HDF5_samples_num, total_num_seq_signals, 2500),\n",
    "                              dtype=\"int\", data=None)\n",
    "\n",
    "            # get labels\n",
    "            print(\"retrieving labels\")\n",
    "            labels = data_matrix_with_neg_and_pos[:, -1]\n",
    "            labels = labels.astype(int)\n",
    "            hf.create_dataset('labels', data=labels[:HDF5_samples_num])\n",
    "\n",
    "            # re_region_matrices = []\n",
    "            for data_count, data_sample in enumerate(data_matrix_with_neg_and_pos[:HDF5_samples_num]):\n",
    "                #print(data_count, \" retrieving regulatory element region matrices\")\n",
    "                chr_num = data_sample[0]\n",
    "                #print('chr_num:',chr_num)\n",
    "                re_region_s = data_sample[1]\n",
    "                #print('re_region_s:',re_region_s)\n",
    "                re_region_e = data_sample[2]\n",
    "                #print('re_region_e:',re_region_e)\n",
    "\n",
    "                re_region_matrix = self.create_matrix(str(chr_num), re_region_s, re_region_e,using_fasta_seq)\n",
    "                #print('re_region_matrix: ', re_region_matrix)\n",
    "                with h5py.File(h5_filename, 'a') as hf:\n",
    "                    #print('hf:', hf)\n",
    "                    hf[\"re_region_matrices\"][data_count] = re_region_matrix\n",
    "\n",
    "            for data_count, data_sample in enumerate(data_matrix_with_neg_and_pos[:HDF5_samples_num]):\n",
    "                #print(data_count, \" retrieving interactions region matrices\")\n",
    "                chr_num = data_sample[0]\n",
    "                in_region_s = data_sample[3]\n",
    "                in_region_e = data_sample[4]\n",
    "\n",
    "                interactions_region_matrix = self.create_matrix(str(chr_num), in_region_s, in_region_e,using_fasta_seq)\n",
    "                with h5py.File(h5_filename, 'a') as hf:\n",
    "                    hf[\"interactions_region_matrices\"][data_count] = interactions_region_matrix\n",
    "\n",
    "            hf.close()\n",
    "\n",
    "            print(\"done creating HDF5 file\")\n",
    "\n",
    "        return h5_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32edd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call init of the class\n",
    "Preprocess(\"../data/\", \"k4me_2500,k27me_2500\", 4, 202344, True, window_size=2500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
